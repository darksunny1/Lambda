# -*- coding: utf-8 -*-
"""svJSPL.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1qRp11yQZ513XAonygJSfvvEBPGZIZ0EM
"""



import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
import operator as op

data = pd.read_excel("jspl.xlsx")

data.head()

data=data[['LOAD', 'VACUUM-1', 'LP1 EXH\n HOOD TMP', 'COND-A CW \nI/L TMP', 'COND-A CW\n O/L TMP']]
data

#rearranging the position of columns
data=data.iloc[:, [0,2,3,4,1]]
data

new_data=data.copy()

new_data.info()

new_data=new_data.iloc[1:,:]
new_data

new_data['VACUUM-1']=new_data['VACUUM-1'].astype(float)
new_data['VACUUM-2']=new_data['VACUUM-1'].astype(float)

new_data['VACUUM-1'].values

new_data['VACUUM-1']=new_data['VACUUM-1'].apply(lambda x: 0 if x<0.0726 else 1)

new_data.head()

new_data['VACUUM-1'].value_counts()

x = new_data.iloc[1:, :-1]
y = new_data.iloc[1:, -1]
x

a = x.iloc[1:, :-1].values
b = x.iloc[1:,  -1].values
a

from sklearn.model_selection import train_test_split
X_train, X_test, Y_train, Y_test = train_test_split(a, b, test_size=0.2, random_state=2)

"""**LOGISTIC REGRESSION**"""

from sklearn.linear_model import LinearRegression
regression = LinearRegression()
regression.fit(X_train, Y_train)

pred = regression.predict(X_test)
np.set_printoptions(precision=2)
print(np.concatenate((pred.reshape(len(pred), 1), Y_test.reshape(len(Y_test), 1)), 1))

pred = (pred > 0.5)
from sklearn.metrics import confusion_matrix
cm=confusion_matrix(Y_test, pred)
print("confusion_matrix:")
print(cm)

import seaborn as sns
import matplotlib.pyplot as plt

fig, ax = plt.subplots(figsize=(8, 8))
ax.matshow(cm, cmap="Blues", alpha=0.3)
cbar_kws={"orientation":"vertical", "label":"color bar"},
for i in range(cm.shape[0]):
  for j in range(cm.shape[1]):
     ax.text(x=j, y=i, s=cm[i, j], va='center', ha='center', size='xx-large')
plt.xlabel('Predictions', fontsize=10)
plt.ylabel('Actual', fontsize=10)
plt.title('Confusion Matrix of Logistic Regression', fontsize='20')
plt.show()

from sklearn.metrics import confusion_matrix
from sklearn.metrics import balanced_accuracy_score
from sklearn.metrics import roc_curve, auc
import matplotlib.pyplot as plt
import numpy as np
y_hat = regression.predict(X_train)
# gets the ROC
fpr, tpr, thresholds = roc_curve(Y_train, y_hat)
roc_auc = auc(fpr, tpr)
# plots ROC
fig = plt.figure(figsize=(10,6))
plt.plot(fpr, tpr, color='#785ef0',
label='ROC curve (AUC = %0.2f)' % roc_auc)
plt.plot([0, 1], [0, 1], color='#dc267f', linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Roc of Logistic Regression', fontsize=20)
plt.legend(loc="lower right")
plt.show()

"""**KNN**"""

#feature Scaling
from sklearn.preprocessing import StandardScaler
st_x= StandardScaler()
X_train= st_x.fit_transform(X_train)
X_test= st_x.transform(X_test)

#Fitting K-NN classifier to the training set
from sklearn.neighbors import KNeighborsClassifier
classifier= KNeighborsClassifier(n_neighbors=5, metric='minkowski', p=2 )

classifier.fit(X_train, Y_train)
y_pred= classifier.predict(X_test)
y_pred = (y_pred > 0.5)
from sklearn.metrics import confusion_matrix
ca=confusion_matrix(Y_test, y_pred)
print("confusion_matrix:")
print(ca)

import seaborn as sns
import matplotlib.pyplot as plt

fig, ax = plt.subplots(figsize=(8, 8))
ax.matshow(ca, cmap="Blues", alpha=0.3)
cbar_kws={"orientation":"vertical", "label":"color bar"},
for i in range(ca.shape[0]):
  for j in range(ca.shape[1]):
     ax.text(x=j, y=i, s=ca[i, j], va='center', ha='center', size='xx-large')
plt.xlabel('Predictions', fontsize=10)
plt.ylabel('Actual', fontsize=10)
plt.title('Confusion Matrix of KNN', fontsize='20')
plt.show()

from sklearn.metrics import confusion_matrix
from sklearn.metrics import balanced_accuracy_score
from sklearn.metrics import roc_curve, auc
import matplotlib.pyplot as plt
import numpy as np
y_hat = classifier.predict(X_train)
# gets the ROC
fpr, tpr, thresholds = roc_curve(Y_train, y_hat)
roc_auc = auc(fpr, tpr)
# plots ROC
fig = plt.figure(figsize=(10,6))
plt.plot(fpr, tpr, color='#785ef0',
label='ROC curve (AUC = %0.2f)' % roc_auc)
plt.plot([0, 1], [0, 1], color='#dc267f', linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC OF KNN', fontsize=20)
plt.legend(loc="lower right")
plt.show()

"""**Random Forest**"""

from sklearn.ensemble import RandomForestClassifier

RFC = RandomForestClassifier(random_state=0)
RFC.fit(X_train, Y_train)

pred_rfc = RFC.predict(X_test)
pred_rfc = (pred_rfc > 0.5)
from sklearn.metrics import confusion_matrix
cw=confusion_matrix(Y_test, pred_rfc)
print("confusion_matrix:")
print(cw)

import seaborn as sns
import matplotlib.pyplot as plt

fig, ax = plt.subplots(figsize=(8, 8))
ax.matshow(cw, cmap="Blues", alpha=0.3)
cbar_kws={"orientation":"vertical", "label":"color bar"},
for i in range(cw.shape[0]):
  for j in range(cw.shape[1]):
     ax.text(x=j, y=i, s=cw[i, j], va='center', ha='center', size='xx-large')
plt.xlabel('Predictions', fontsize=10)
plt.ylabel('Actual', fontsize=10)
plt.title('Confusion Matrix of Random Forest', fontsize='20')
plt.show()

from sklearn.metrics import confusion_matrix
from sklearn.metrics import balanced_accuracy_score
from sklearn.metrics import roc_curve, auc
import matplotlib.pyplot as plt
import numpy as np
y_hat = RFC.predict(X_train)
# gets the ROC
fpr, tpr, thresholds = roc_curve(Y_train, y_hat)
roc_auc = auc(fpr, tpr)
# plots ROC
fig = plt.figure(figsize=(10,6))
plt.plot(fpr, tpr, color='#785ef0',
label='ROC curve (AUC = %0.2f)' % roc_auc)
plt.plot([0, 1], [0, 1], color='#dc267f', linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC OF Random Forest', fontsize=20)
plt.legend(loc="lower right")
plt.show()

"""**SVM**"""

from sklearn import svm
from sklearn.svm import SVC

model_3 = svm.SVC()
model_3.fit(X_train,Y_train)

from sklearn.metrics import confusion_matrix
from sklearn.metrics import balanced_accuracy_score
from sklearn.metrics import roc_curve, auc
import matplotlib.pyplot as plt
pred_5 = model_3.predict(X_test)
pred_5 = (pred_5 > 0.5)
cc=confusion_matrix(Y_test, pred_5)
print("confusion_matrix:")
print(cc)

import seaborn as sns
import matplotlib.pyplot as plt

fig, ax = plt.subplots(figsize=(8, 8))
ax.matshow(cc, cmap="Blues", alpha=0.3)
cbar_kws={"orientation":"vertical", "label":"color bar"},
for i in range(cc.shape[0]):
  for j in range(cc.shape[1]):
     ax.text(x=j, y=i, s=cc[i, j], va='center', ha='center', size='xx-large')
plt.xlabel('Predictions', fontsize=10)
plt.ylabel('Actual', fontsize=10)
plt.title('Confusion Matrix of SVM', fontsize='20')
plt.show()

from sklearn.metrics import confusion_matrix
from sklearn.metrics import balanced_accuracy_score
from sklearn.metrics import roc_curve, auc
import matplotlib.pyplot as plt
import numpy as np
y_hat = model_3.predict(X_train)
# gets the ROC
fpr, tpr, thresholds = roc_curve(Y_train, y_hat)
roc_auc = auc(fpr, tpr)
# plots ROC
fig = plt.figure(figsize=(10,6))
plt.plot(fpr, tpr, color='#785ef0',
label='ROC curve (AUC = %0.2f)' % roc_auc)
plt.plot([0, 1], [0, 1], color='#dc267f', linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC OF SVM', fontsize=20)
plt.legend(loc="lower right")
plt.show()

